{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 185
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 48280,
     "status": "ok",
     "timestamp": 1568166190977,
     "user": {
      "displayName": "Dmitry Vengertsev",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDYD27Z8EhBcFDMesTklMQKc27chgzd92EokKwqJw=s64",
      "userId": "11039444983797551480"
     },
     "user_tz": 360
    },
    "id": "ihKjA9UrM_kO",
    "outputId": "dc00057b-c043-4a3f-92f7-a5d8277697e0"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Create ground truth with all states pre-calculated\n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import marisa_trie\n",
    "# https://stackoverflow.com/questions/11015320/how-to-create-a-trie-in-python\n",
    "import glob\n",
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os \n",
    "\n",
    "import h5py\n",
    "\n",
    "# ground truth parameters\n",
    "char_seq_len=10\n",
    "delta_step=1\n",
    "# Configs\n",
    "data_folder = \"data/\"\n",
    "train_dataset = data_folder + \"training_dataset/nietzsche.txt\"\n",
    "folder_models = data_folder + \"trained_models/\"\n",
    "output_ground_truth = data_folder + \"ground_truth/\"\n",
    "\n",
    "\n",
    "class CharRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, tokens, n_hidden=612, n_layers=4,\n",
    "                               drop_prob=0.5, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        \n",
    "        # creating character dictionaries\n",
    "        self.chars = tokens\n",
    "        self.int2char = dict(enumerate(self.chars))\n",
    "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
    "        \n",
    "        ## TODO: define the LSTM\n",
    "        self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        ## TODO: define a dropout layer\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "        ## TODO: define the final, fully-connected output layer\n",
    "        self.fc = nn.Linear(n_hidden, len(self.chars))\n",
    "      \n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        ''' Forward pass through the network. \n",
    "            These inputs are x, and the hidden/cell state `hidden`. '''\n",
    "                \n",
    "        ## TODO: Get the outputs and the new hidden state from the lstm\n",
    "        r_output, hidden = self.lstm(x, hidden)\n",
    "        \n",
    "        ## TODO: pass through a dropout layer\n",
    "        out = self.dropout(r_output)\n",
    "        \n",
    "        # Stack up LSTM outputs using view\n",
    "        # you may need to use contiguous to reshape the output\n",
    "        out = out.contiguous().view(-1, self.n_hidden)\n",
    "        \n",
    "        ## TODO: put x through the fully-connected layer\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        # return the final output and the hidden state\n",
    "        return out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
    "        \n",
    "        return hidden\n",
    "      \n",
    "      \n",
    "def one_hot_encode(arr, n_labels):\n",
    "    \n",
    "    # Initialize the the encoded array\n",
    "    one_hot = np.zeros((np.multiply(*arr.shape), n_labels), dtype=np.float32)\n",
    "    \n",
    "    # Fill the appropriate elements with ones\n",
    "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
    "    \n",
    "    # Finally reshape it to get back to the original array\n",
    "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
    "    \n",
    "    return one_hot\n",
    "  \n",
    "\n",
    "def predict(net, char, h=None, top_k=None, return_vectors=True):\n",
    "        ''' Given a character, predict the next character.\n",
    "            Returns the predicted character and the hidden state.\n",
    "        '''\n",
    "        \n",
    "        # tensor inputs\n",
    "        x = np.array([[net.char2int[char]]])\n",
    "        x = one_hot_encode(x, len(net.chars))\n",
    "        inputs = torch.from_numpy(x)\n",
    "        \n",
    "        if(train_on_gpu):\n",
    "            inputs = inputs.cuda()\n",
    "        \n",
    "        # detach hidden state from history\n",
    "        # print(\"h={}\".format(h))\n",
    "        # print(\"len h {}\".format(len(h)))\n",
    "        h = tuple([each.data for each in h])\n",
    "        # get the output of the model\n",
    "        \n",
    "        # print(\"Is CUDA: {}\".format(inputs.is_cuda))\n",
    "        # inputs.is_cuda\n",
    "\n",
    "        \n",
    "        out, h = net(inputs, h)\n",
    "\n",
    "        # get the character probabilities\n",
    "        # apply softmax to get p probabilities for the likely next character giving x\n",
    "        p = F.softmax(out, dim=1).data\n",
    "        if(train_on_gpu):\n",
    "            p = p.cpu() # move to cpu\n",
    "        \n",
    "        \n",
    "        # keep probability to use later\n",
    "        prob = p.numpy().squeeze()\n",
    "        \n",
    "        if not return_vectors:        \n",
    "          char_index = np.argmax(prob)\n",
    "          char_pred = net.int2char[char_index]\n",
    "          prob_pred = prob[char_index]\n",
    "          return char_pred, h, prob_pred \n",
    "        else: \n",
    "          return h, prob       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 48256,
     "status": "ok",
     "timestamp": 1568166190979,
     "user": {
      "displayName": "Dmitry Vengertsev",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDYD27Z8EhBcFDMesTklMQKc27chgzd92EokKwqJw=s64",
      "userId": "11039444983797551480"
     },
     "user_tz": 360
    },
    "id": "3ZzWDzobYiKb",
    "outputId": "98f8cfd9-c0ff-4343-a86b-fbd24b7e8eff"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") \n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 54441,
     "status": "ok",
     "timestamp": 1568166197182,
     "user": {
      "displayName": "Dmitry Vengertsev",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDYD27Z8EhBcFDMesTklMQKc27chgzd92EokKwqJw=s64",
      "userId": "11039444983797551480"
     },
     "user_tz": 360
    },
    "id": "QbDT7nln4_BI",
    "outputId": "787879ab-2cb8-4ae9-c456-f8c991852fee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** Training data processing - Started. ***************\n",
      "content_size = 580741 tokens_max_cnt = 580741\n",
      "Example of tokens ['use the la', 'od grounds', ' sinful in']\n",
      "tokens_incr = 1418404\n",
      "Example of tokens incr [' one sta', 'inal enoug', 'oohuman f', ' asses', 'esses his ', 'and goes', 'the tedi', 'him  114 t', 'ianism by ', 'efic', 'as life', ' victim', 'f denial', 'nes acts ', 'bted first', 'ng disposi', 'lish eq', 'ly losing', 'to so emph', 'emocrats', 'eoise ju', 'he spee', ' its def', 'ison and a', 'ctions pr', 'er v the n', 'ape obedie', 'w of hu', 'angled to', 'ropical l']\n",
      "Min token len: 1\n",
      "Max token len: 10\n",
      "Avg token len: 8.26963051429635\n",
      "********** Training data processing - Completed. ***************\n",
      "********** Construct Ground Truth Data Structure=Prefix tree - Started. ***************\n",
      "['s', 'se', 'see', 'see ', 'see m', 'see mo']\n",
      "********** Construct Ground Truth Data Structure=Prefix tree - Completed. ***************\n",
      "model_fname_list: ['data/trained_models\\\\model_1024_1_160_50', 'data/trained_models\\\\model_1024_1_80_50', 'data/trained_models\\\\model_1024_4_80_50', 'data/trained_models\\\\model_124_1_160_50', 'data/trained_models\\\\model_124_1_80_50', 'data/trained_models\\\\model_124_4_160_50', 'data/trained_models\\\\model_124_4_80_50', 'data/trained_models\\\\model_512_1_160_50', 'data/trained_models\\\\model_512_1_80_50', 'data/trained_models\\\\model_512_4_160_50', 'data/trained_models\\\\model_512_4_80_50']\n"
     ]
    }
   ],
   "source": [
    "print(\"********** Training data processing - Started. ***************\")\n",
    "f = open(train_dataset, 'r')\n",
    "content = f.read()\n",
    "import re\n",
    "s = \"string. With. Punctuation?\"\n",
    "s = re.sub(r'[^\\w\\s]','',content)\n",
    "s = re.sub('\\n', ' ', s)\n",
    "s = str.lower(s)\n",
    "content = s\n",
    "\n",
    "content_size = len(content)\n",
    "tokens_max_cnt = int(content_size/delta_step)\n",
    "print('content_size = {} tokens_max_cnt = {}'.format(content_size, tokens_max_cnt))\n",
    "\n",
    "tokens_all = set()\n",
    "for i in range(0, tokens_max_cnt):  \n",
    "  start = int(i*delta_step)\n",
    "  end = int(i*delta_step + char_seq_len)\n",
    "  # print('start = {}, end = {}'.format(start, end))  \n",
    "  seq = content[start:end]\n",
    "  # print('seq = {}'.format(seq))\n",
    "  tokens_all.add(seq)\n",
    "  \n",
    "tokens = list(tokens_all)\n",
    "print('Example of tokens {}'.format(tokens[0:3]))\n",
    "\n",
    "tokens_incr = set() # incremental tokens\n",
    "\n",
    "for token in tokens:\n",
    "  for i in range(0, len(token)+1):\n",
    "    token_i = token[0: i]\n",
    "    #print(token_i)\n",
    "    tokens_incr.add(token_i)\n",
    "tokens_incr = list(tokens_incr)\n",
    "tokens_incr.remove('')\n",
    "print(\"tokens_incr = {}\".format(len(tokens_incr)))\n",
    "print('Example of tokens incr {}'.format(tokens_incr[0:30]))\n",
    "tokens_len = [ len(x) for x in tokens_incr]\n",
    "print(\"Min token len: {}\".format(np.min(tokens_len)))\n",
    "print(\"Max token len: {}\".format(np.max(tokens_len)))\n",
    "print(\"Avg token len: {}\".format(np.mean(tokens_len)))\n",
    "print(\"********** Training data processing - Completed. ***************\")\n",
    "\n",
    "\n",
    "print(\"********** Construct Ground Truth Data Structure=Prefix tree - Started. ***************\")\n",
    "trie = marisa_trie.Trie(tokens_incr)\n",
    "print(trie.prefixes('see mo'))\n",
    "# object_methods = [method_name for method_name in dir(trie)\n",
    "#                   if callable(getattr(trie, method_name))]\n",
    "# object_methods\n",
    "print(\"********** Construct Ground Truth Data Structure=Prefix tree - Completed. ***************\")\n",
    "\n",
    "# 1. get list of models from a directory\n",
    "\n",
    "model_fname_list = glob.glob(folder_models+\"/*\")\n",
    "print(\"model_fname_list: {}\".format(model_fname_list))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 171
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 60049,
     "status": "ok",
     "timestamp": 1568166202820,
     "user": {
      "displayName": "Dmitry Vengertsev",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDYD27Z8EhBcFDMesTklMQKc27chgzd92EokKwqJw=s64",
      "userId": "11039444983797551480"
     },
     "user_tz": 360
    },
    "id": "1nK5-S9gZ9eP",
    "outputId": "e8d86dcb-8787-41b2-df9c-9fe90b1117c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** model_fname = data/trained_models/model_124_1_80_50 *********\n",
      "Model info: CharRNN(\n",
      "  (lstm): LSTM(44, 124, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc): Linear(in_features=124, out_features=44, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_fname = 'data/trained_models/model_124_1_80_50'\n",
    "model_base_name = os.path.basename(model_fname)\n",
    "\n",
    "\n",
    "print(\"********** model_fname = {} *********\".format(model_fname))\n",
    "with open(model_fname, 'rb') as f:\n",
    "      checkpoint = torch.load(f)\n",
    "model = CharRNN(checkpoint['tokens'], n_hidden=checkpoint['n_hidden'], n_layers=checkpoint['n_layers'])\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "print(\"Model info: {}\".format(model))\n",
    "\n",
    "  # Add for-loop for elements of tokens_incr\n",
    "  \n",
    "  # 2. predict Y and H for all elements of marisa-trie\n",
    "  # 3. save as trie? or dict?!!!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4aTnGYUEfjrx"
   },
   "outputs": [],
   "source": [
    "# Ground Truth\n",
    "class GroundTruth :\n",
    "  \n",
    "  def __init__(self, _trie, _ymap, _hmap):\n",
    "    self.trie = _trie\n",
    "    self.ymap = _ymap # mapping of trie node ID to y variable\n",
    "    self.hmap = _hmap # mapping of trie node ID to hidden state variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 59994,
     "status": "ok",
     "timestamp": 1568166202822,
     "user": {
      "displayName": "Dmitry Vengertsev",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDYD27Z8EhBcFDMesTklMQKc27chgzd92EokKwqJw=s64",
      "userId": "11039444983797551480"
     },
     "user_tz": 360
    },
    "id": "zWVuvNjuufqY",
    "outputId": "0a1fc95b-e7a7-4421-b6c9-530396cf9add"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(model.parameters()).is_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 60058,
     "status": "ok",
     "timestamp": 1568166202908,
     "user": {
      "displayName": "Dmitry Vengertsev",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDYD27Z8EhBcFDMesTklMQKc27chgzd92EokKwqJw=s64",
      "userId": "11039444983797551480"
     },
     "user_tz": 360
    },
    "id": "DOtP1Z34JEYM",
    "outputId": "be26adeb-7dc5-4e37-ecbb-3bf71490bfe4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = model.to(device)\n",
    "next(model.parameters()).is_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i33MILmQyv9J"
   },
   "outputs": [],
   "source": [
    "ymap = {}\n",
    "hmap = {}\n",
    "train_on_gpu = True\n",
    "\n",
    "def predict_seq(_chars, _model):\n",
    "  h = model.init_hidden(1)\n",
    "  for ch in _chars:\n",
    "    h, prob = predict(model, ch, h)\n",
    "    # print(\"ch = {}, char = {}, prob = {}\".format(ch, char, prob))    \n",
    "  return h, prob      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "trie.save(output_ground_truth+'/gt_'+model_base_name+'.marisa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "part = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = h5py.File(output_ground_truth+'/gt_'+model_base_name+'_'+str(part)+'_states.h5py', 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "trie_keys = trie.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = len(trie_keys)/4.0\n",
    "start = round((part-1)*delta)\n",
    "end = round(part*delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part:1, start=0, end=354601\n"
     ]
    }
   ],
   "source": [
    "print(\"part:{}, start={}, end={}\".format(part, start, end))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "354601"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "element_ids = [x[1] for x in trie.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 40, 788, 6745, 30956, 93672, 215776, 409317, 678428, 1018535]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "element_ids[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "354601"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trie.items()[start : end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(' ', 0),\n",
       " (' a', 40),\n",
       " (' an', 788),\n",
       " (' and', 6745),\n",
       " (' and ', 30956),\n",
       " (' and s', 93672),\n",
       " (' and se', 215776),\n",
       " (' and sel', 409317),\n",
       " (' and self', 678428),\n",
       " (' and self ', 1018535)]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trie.items()[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elem =  if one i. k = 50000\n",
      "elem =  mankinda. k = 100000\n",
      "elem =  remain g. k = 150000\n",
      "elem = e left af. k = 200000\n",
      "elem = eserts. k = 250000\n",
      "elem = ey are alr. k = 300000\n",
      "elem = alsest op. k = 350000\n"
     ]
    }
   ],
   "source": [
    "# using hdf5\n",
    "k = 0\n",
    "for elem, elem_id in trie.items()[start : end]:\n",
    "  h, prob = predict_seq(elem, model)\n",
    "  f['y_'+str(elem_id)] = prob\n",
    "  f['h_'+str(elem_id)] = h[0].data.cpu().numpy() # only the values, do not save gradient\n",
    "\n",
    "  k=k+1\n",
    "  if (k%50000==0):\n",
    "    print(\"elem = {}. k = {}\".format(elem, k))\n",
    "  # if k>10:\n",
    "    # break\n",
    "  \n",
    "# model_gt = GroundTruth(trie, ymap, hmap)\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read  and check that order of elems is the same\n",
    "g = h5py.File(output_ground_truth+'/gt_'+model_base_name+'_'+str(part)+'_states.h5py', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ',\n",
       " ' a',\n",
       " ' an',\n",
       " ' and',\n",
       " ' and ',\n",
       " ' and s',\n",
       " ' and se',\n",
       " ' and sel',\n",
       " ' and self',\n",
       " ' and self ']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trie_keys[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_trie_keys = list(g.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.15970162e-02, 1.02252125e-04, 3.18567851e-04, 1.17493124e-04,\n",
       "       4.23897393e-02, 3.73146759e-04, 2.39318255e-02, 8.48555639e-02,\n",
       "       9.20189632e-05, 9.47463363e-02, 2.53079622e-03, 4.12827125e-03,\n",
       "       3.13363667e-03, 1.02065147e-04, 2.29419004e-02, 9.67583582e-02,\n",
       "       7.77364075e-02, 7.15183513e-03, 5.76696023e-02, 1.02002057e-04,\n",
       "       4.92777872e-05, 4.79606800e-02, 1.94603726e-02, 9.63516650e-05,\n",
       "       1.07069046e-03, 2.20848620e-03, 1.93535147e-04, 6.43221214e-02,\n",
       "       7.88175687e-02, 8.63932510e-05, 1.41437287e-02, 7.15470524e-05,\n",
       "       9.15253913e-05, 4.38000150e-02, 3.91744189e-02, 3.57919931e-02,\n",
       "       7.22919474e-04, 3.01809353e-03, 5.95195706e-05, 5.39685041e-02,\n",
       "       1.87969841e-02, 1.45215364e-02, 7.59283954e-04, 3.56424353e-05],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g['y_1018535'][...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 6.4322e-03, -2.8410e-02,  3.1460e-01,  7.0690e-02, -9.0526e-02,\n",
       "           8.5714e-02,  8.1651e-02,  3.4326e-02, -1.8042e-01, -1.6631e-01,\n",
       "          -6.0355e-03, -3.2680e-01, -2.2718e-02, -9.7644e-02,  4.8761e-02,\n",
       "           1.6566e-02, -4.8820e-02, -4.6266e-02, -9.4683e-02,  1.4593e-01,\n",
       "           8.7851e-04,  2.4238e-02,  5.9539e-02,  5.5022e-03, -5.8738e-02,\n",
       "          -4.3814e-02,  2.1783e-03,  2.1565e-02,  1.0630e-02,  8.2591e-03,\n",
       "           7.4929e-03, -1.8718e-02,  2.4823e-02,  8.5742e-03, -2.8612e-02,\n",
       "          -1.0967e-01, -1.6261e-02,  4.1212e-02, -2.2671e-02, -1.4108e-01,\n",
       "          -8.6997e-02, -6.7066e-02, -2.5027e-03,  1.4641e-02,  1.0700e-01,\n",
       "           1.4883e-01, -3.8611e-02, -1.8133e-01,  8.4864e-03,  8.8618e-02,\n",
       "           7.8809e-03,  4.2759e-02,  8.3621e-02,  1.2503e-01,  1.3461e-02,\n",
       "          -2.0543e-02, -1.1269e-02,  1.3726e-01,  6.8421e-02, -9.7921e-02,\n",
       "           6.9353e-02, -2.8590e-01,  1.6102e-02, -6.4238e-02, -1.6368e-01,\n",
       "           2.4099e-04,  4.9696e-03, -1.0978e-02, -4.8460e-02,  3.4204e-01,\n",
       "          -7.3963e-03, -7.8811e-04,  3.0053e-02, -1.7919e-02, -2.5685e-01,\n",
       "           3.1968e-02,  8.9834e-03,  4.8901e-02, -3.8977e-02,  7.5414e-02,\n",
       "           4.3533e-02, -8.9039e-03, -3.5974e-02,  8.7315e-03,  5.6046e-02,\n",
       "          -4.1594e-01, -8.5867e-03,  2.0591e-02,  5.6837e-02, -1.9851e-03,\n",
       "          -1.2418e-01, -4.1532e-02,  6.9892e-02,  5.7283e-03,  2.5854e-01,\n",
       "          -1.1525e-02,  3.6749e-02, -7.5628e-02,  9.3960e-04,  2.6819e-02,\n",
       "          -1.0770e-02,  1.7857e-03,  1.5432e-02,  7.0618e-02,  5.7759e-02,\n",
       "           3.4828e-03,  3.7220e-01,  2.5265e-02, -9.0705e-02, -3.2038e-02,\n",
       "           8.8519e-02,  5.5480e-04, -1.4751e-01,  9.6564e-02,  5.2697e-03,\n",
       "          -8.7525e-03, -2.0831e-01,  4.3396e-02, -2.5460e-02,  7.0288e-03,\n",
       "           1.9336e-02, -1.1268e-02,  4.1456e-01,  4.0579e-02]]],\n",
       "       device='cuda:0', grad_fn=<CudnnRnnBackward>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "f['h_0'] = prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using pickle\n",
    "\n",
    "k = 0\n",
    "for elem in trie:\n",
    "  #print(\"elem = {}. k = {}\".format(elem, k))\n",
    "  # print(\"elem len = {}\".format(len(elem)))\n",
    "  h, prob = predict_seq(elem, model)\n",
    "  elem_id = trie[elem]\n",
    "  ymap[elem_id] = prob\n",
    "  hmap[elem_id] = h\n",
    "  k=k+1\n",
    "  if (k%50000==0):\n",
    "    print(\"elem = {}. k = {}\".format(elem, k))\n",
    "  # if k>10:\n",
    "    # break\n",
    "  \n",
    "# model_gt = GroundTruth(trie, ymap, hmap)\n",
    "\n",
    "with open(output_ground_truth+'/gt_'+model_base_name+'.pickle', 'wb') as handle:\n",
    "    pickle.dump(model_gt, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "calc_ground_truth.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
