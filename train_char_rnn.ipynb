{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SD6jBKxuSbrc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTraining and saving models\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Training and saving models\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.3\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RqIjYfxlSdpJ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import time\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_H9TD_kVFg1n"
   },
   "outputs": [],
   "source": [
    "# Configs\n",
    "data_folder = \"data/\"\n",
    "train_dataset = data_folder + \"training_dataset/nietzsche.txt\"\n",
    "output_folder_models = data_folder + \"trained_models/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MGaAFjVzHVkG"
   },
   "outputs": [],
   "source": [
    "# Main functions\n",
    "\n",
    "def one_hot_encode(arr, n_labels):\n",
    "    \n",
    "    # Initialize the the encoded array\n",
    "    one_hot = np.zeros((np.multiply(*arr.shape), n_labels), dtype=np.float32)\n",
    "    \n",
    "    # Fill the appropriate elements with ones\n",
    "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
    "    \n",
    "    # Finally reshape it to get back to the original array\n",
    "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
    "    \n",
    "    return one_hot\n",
    "  \n",
    "def get_batches(arr, batch_size, seq_length):\n",
    "  '''Create a generator that returns batches of size\n",
    "     batch_size x seq_length from arr.\n",
    "\n",
    "     Arguments\n",
    "     ---------\n",
    "     arr: Array you want to make batches from\n",
    "     batch_size: Batch size, the number of sequences per batch\n",
    "     seq_length: Number of encoded chars in a sequence\n",
    "  '''\n",
    "\n",
    "  batch_size_total = batch_size * seq_length\n",
    "  # total number of batches we can make, // integer division, round down\n",
    "  n_batches = len(arr)//batch_size_total\n",
    "\n",
    "  # Keep only enough characters to make full batches\n",
    "  arr = arr[:n_batches * batch_size_total]\n",
    "  # Reshape into batch_size rows, n. of first row is the batch size, the other lenght is inferred\n",
    "  arr = arr.reshape((batch_size, -1))\n",
    "\n",
    "  # iterate through the array, one sequence at a time\n",
    "  for n in range(0, arr.shape[1], seq_length):\n",
    "      # The features\n",
    "      x = arr[:, n:n+seq_length]\n",
    "      # The targets, shifted by one\n",
    "      y = np.zeros_like(x)\n",
    "      try:\n",
    "          y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n",
    "      except IndexError:\n",
    "          y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
    "      yield x, y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n-j2F6AHHVhU"
   },
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, tokens, n_hidden=612, n_layers=4,\n",
    "                               drop_prob=0.5, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        \n",
    "        # creating character dictionaries\n",
    "        self.chars = tokens\n",
    "        self.int2char = dict(enumerate(self.chars))\n",
    "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
    "        \n",
    "        ## TODO: define the LSTM\n",
    "        self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        ## TODO: define a dropout layer\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "        ## TODO: define the final, fully-connected output layer\n",
    "        self.fc = nn.Linear(n_hidden, len(self.chars))\n",
    "      \n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        ''' Forward pass through the network. \n",
    "            These inputs are x, and the hidden/cell state `hidden`. '''\n",
    "                \n",
    "        ## TODO: Get the outputs and the new hidden state from the lstm\n",
    "        r_output, hidden = self.lstm(x, hidden)\n",
    "        \n",
    "        ## TODO: pass through a dropout layer\n",
    "        out = self.dropout(r_output)\n",
    "        \n",
    "        # Stack up LSTM outputs using view\n",
    "        # you may need to use contiguous to reshape the output\n",
    "        out = out.contiguous().view(-1, self.n_hidden)\n",
    "        \n",
    "        ## TODO: put x through the fully-connected layer\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        # return the final output and the hidden state\n",
    "        return out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
    "        \n",
    "        return hidden\n",
    "      \n",
    "      \n",
    "def train(net, data, epochs=10, batch_size=10, seq_length=50, lr=0.001, clip=5, val_frac=0.1, print_every=10):\n",
    "    ''' Training a network \n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        \n",
    "        net: CharRNN network\n",
    "        data: text data to train the network\n",
    "        epochs: Number of epochs to train\n",
    "        batch_size: Number of mini-sequences per mini-batch, aka batch size\n",
    "        seq_length: Number of character steps per mini-batch\n",
    "        lr: learning rate\n",
    "        clip: gradient clipping\n",
    "        val_frac: Fraction of data to hold out for validation\n",
    "        print_every: Number of steps for printing training and validation loss\n",
    "    \n",
    "    '''\n",
    "    net.train()\n",
    "    \n",
    "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # create training and validation data\n",
    "    val_idx = int(len(data)*(1-val_frac))\n",
    "    data, val_data = data[:val_idx], data[val_idx:]\n",
    "    \n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "    \n",
    "    counter = 0\n",
    "    n_chars = len(net.chars)\n",
    "    for e in range(epochs):\n",
    "        # initialize hidden state\n",
    "        h = net.init_hidden(batch_size)\n",
    "        \n",
    "        for x, y in get_batches(data, batch_size, seq_length):\n",
    "            counter += 1\n",
    "            \n",
    "            # One-hot encode our data and make them Torch tensors\n",
    "            x = one_hot_encode(x, n_chars)\n",
    "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "            targets = targets.long() \n",
    "            #print(\"inputs={}\".format(inputs))\n",
    "            #print(\"targets={}\".format(targets))            \n",
    "            \n",
    "            if(train_on_gpu):\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "                \n",
    "            # Creating new variables for the hidden state, otherwise\n",
    "            # we'd backprop through the entire training history\n",
    "            h = tuple([each.data for each in h])\n",
    "\n",
    "            # zero accumulated gradients\n",
    "            net.zero_grad()\n",
    "            \n",
    "            # get the output from the model\n",
    "            output, h = net(inputs, h)\n",
    "            \n",
    "            # calculate the loss and perform backprop\n",
    "            loss = criterion(output, targets.view(batch_size*seq_length))\n",
    "            loss.backward()\n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "            opt.step()\n",
    "            \n",
    "            # loss stats\n",
    "            if counter % print_every == 0:\n",
    "                # Get validation loss\n",
    "                val_h = net.init_hidden(batch_size)\n",
    "                val_losses = []\n",
    "                net.eval()\n",
    "                for x, y in get_batches(val_data, batch_size, seq_length):\n",
    "                    # One-hot encode our data and make them Torch tensors\n",
    "                    x = one_hot_encode(x, n_chars)\n",
    "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
    "                    \n",
    "                    # Creating new variables for the hidden state, otherwise\n",
    "                    # we'd backprop through the entire training history\n",
    "                    val_h = tuple([each.data for each in val_h])\n",
    "                    \n",
    "                    inputs, targets = x, y\n",
    "                    targets = targets.long() \n",
    "                    if(train_on_gpu):\n",
    "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "                        \n",
    "                    output, val_h = net(inputs, val_h)\n",
    "                    val_loss = criterion(output, targets.view(batch_size*seq_length))\n",
    "                \n",
    "                    val_losses.append(val_loss.item())\n",
    "                \n",
    "                    # auc = net.evaluate_auc(self.batch_size)                \n",
    "                \n",
    "                net.train() # reset to train mode after iterationg through validation data\n",
    "                \n",
    "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
    "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))\n",
    "                #,   \"AUC: {2:0.3f}\".format(np.mean(auc)))\n",
    "                \n",
    "                hist_val_loss.append(np.mean(val_losses))\n",
    "                hist_loss.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 368,
     "status": "ok",
     "timestamp": 1567062323926,
     "user": {
      "displayName": "Dmitry Vengertsev",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDYD27Z8EhBcFDMesTklMQKc27chgzd92EokKwqJw=s64",
      "userId": "11039444983797551480"
     },
     "user_tz": 360
    },
    "id": "M03ARFACSdl6",
    "outputId": "771c06ae-1d0d-47e2-8ec8-1569d85d7a5c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content_size = 593235\n",
      "Example of content: preface   supposing that truth is a woman what then is there not ground for suspecting that all philosophers, in so far as they have been dogmatists, have failed to understand women that the terrible seriousness and clumsy importunity with which they have usually paid their addresses to truth, have been unskilled and unseemly methods for winning a woman certainly she has never allowed herself to be won and at present every kind of dogma stands with sad and discouraged mien if, indeed, it stands at all for there are scoffers who maintain that it has fallen, that all dogma lies on the ground nay more, that it is at its last gasp. but to speak seriously, there are good grounds for hoping that all dogmatizing in philosophy, whatever solemn, whatever conclusive and decided airs it has assumed, may have been only a noble puerilism and tyronism and probably the time is at hand when it will be once and again understood what has actually sufficed for the basis of such imposing and absolute phil\n",
      "[[[0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "  [0. 1. 0. 0. 0. 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "# Data preparation\n",
    "f = open(train_dataset, 'r')\n",
    "content = f.read()\n",
    "import re\n",
    "s = re.sub('--', ' ', content)\n",
    "s = re.sub(r'[^\\w\\s\\,\\.]','',s)\n",
    "s = re.sub('\\n', ' ', s)\n",
    "\n",
    "s = str.lower(s)\n",
    "content = s\n",
    "\n",
    "content_size = len(content)\n",
    "print('content_size = {}'.format(content_size))\n",
    "print('Example of content: {}'.format(content[0:1000]))\n",
    "\n",
    "test_seq = np.array([[3, 5, 1]])\n",
    "one_hot = one_hot_encode(test_seq, 8)\n",
    "\n",
    "print(one_hot)\n",
    "\n",
    "text = content\n",
    "\n",
    "# encode the text and map each character to an integer and vice versa\n",
    "\n",
    "# we create two dictionaries:\n",
    "# 1. int2char, which maps integers to characters\n",
    "# 2. char2int, which maps characters to unique integers\n",
    "chars = tuple(set(text))\n",
    "int2char = dict(enumerate(chars))\n",
    "char2int = {ch: ii for ii, ch in int2char.items()}\n",
    "\n",
    "# encode the text\n",
    "encoded = np.array([char2int[ch] for ch in text])\n",
    "\n",
    "\n",
    "batches = get_batches(encoded, 8, 50)\n",
    "x, y = next(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 310,
     "status": "ok",
     "timestamp": 1567062330301,
     "user": {
      "displayName": "Dmitry Vengertsev",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDYD27Z8EhBcFDMesTklMQKc27chgzd92EokKwqJw=s64",
      "userId": "11039444983797551480"
     },
     "user_tz": 360
    },
    "id": "wPomM06KbXwx",
    "outputId": "6ebd44b7-02ba-4d52-9057-e796faf6ed2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GPU!\n"
     ]
    }
   ],
   "source": [
    "train_on_gpu = torch.cuda.is_available()\n",
    "if(train_on_gpu):\n",
    "    print('Training on GPU!')\n",
    "else: \n",
    "    print('No GPU available, training on CPU; consider making n_epochs very small.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pNgDJZ1UKFib"
   },
   "outputs": [],
   "source": [
    "# Model Training parameters\n",
    "\n",
    "# n_hidden_list = [124, 512, 1024]\n",
    "# n_layers_list = [1,4]\n",
    "# seq_length_list = [80, 160]\n",
    "\n",
    "n_hidden_list = [124]\n",
    "n_layers_list = [4]\n",
    "seq_length_list = [160]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0sVaiCtobXtP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** model_124_4_160_50 ************\n",
      "CharRNN(\n",
      "  (lstm): LSTM(44, 124, num_layers=4, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc): Linear(in_features=124, out_features=44, bias=True)\n",
      ")\n",
      "Epoch: 1/50... Step: 50... Loss: 2.9417... Val Loss: 2.8990\n",
      "Epoch: 2/50... Step: 100... Loss: 2.9173... Val Loss: 2.8940\n",
      "Epoch: 3/50... Step: 150... Loss: 2.9182... Val Loss: 2.8919\n",
      "Epoch: 4/50... Step: 200... Loss: 2.9112... Val Loss: 2.8912\n",
      "Epoch: 5/50... Step: 250... Loss: 2.9004... Val Loss: 2.8907\n",
      "Epoch: 6/50... Step: 300... Loss: 2.9049... Val Loss: 2.8907\n",
      "Epoch: 7/50... Step: 350... Loss: 2.9154... Val Loss: 2.8909\n",
      "Epoch: 8/50... Step: 400... Loss: 2.9097... Val Loss: 2.8908\n",
      "Epoch: 9/50... Step: 450... Loss: 2.8926... Val Loss: 2.8683\n",
      "Epoch: 10/50... Step: 500... Loss: 2.7202... Val Loss: 2.6759\n",
      "Epoch: 11/50... Step: 550... Loss: 2.6332... Val Loss: 2.5757\n",
      "Epoch: 12/50... Step: 600... Loss: 2.5638... Val Loss: 2.4729\n",
      "Epoch: 13/50... Step: 650... Loss: 2.4666... Val Loss: 2.4136\n",
      "Epoch: 14/50... Step: 700... Loss: 2.4359... Val Loss: 2.3680\n",
      "Epoch: 15/50... Step: 750... Loss: 2.4270... Val Loss: 2.3226\n",
      "Epoch: 16/50... Step: 800... Loss: 2.3686... Val Loss: 2.2839\n",
      "Epoch: 17/50... Step: 850... Loss: 2.3557... Val Loss: 2.2585\n",
      "Epoch: 18/50... Step: 900... Loss: 2.3078... Val Loss: 2.2342\n",
      "Epoch: 19/50... Step: 950... Loss: 2.2920... Val Loss: 2.2161\n",
      "Epoch: 20/50... Step: 1000... Loss: 2.2926... Val Loss: 2.1980\n",
      "Epoch: 21/50... Step: 1050... Loss: 2.2589... Val Loss: 2.1822\n",
      "Epoch: 22/50... Step: 1100... Loss: 2.2701... Val Loss: 2.1678\n",
      "Epoch: 23/50... Step: 1150... Loss: 2.2372... Val Loss: 2.1527\n",
      "Epoch: 24/50... Step: 1200... Loss: 2.2490... Val Loss: 2.1413\n",
      "Epoch: 25/50... Step: 1250... Loss: 2.2287... Val Loss: 2.1312\n",
      "Epoch: 25/50... Step: 1300... Loss: 2.1908... Val Loss: 2.1184\n",
      "Epoch: 26/50... Step: 1350... Loss: 2.2113... Val Loss: 2.1061\n",
      "Epoch: 27/50... Step: 1400... Loss: 2.1799... Val Loss: 2.0956\n",
      "Epoch: 28/50... Step: 1450... Loss: 2.1925... Val Loss: 2.0859\n",
      "Epoch: 29/50... Step: 1500... Loss: 2.1890... Val Loss: 2.0809\n",
      "Epoch: 30/50... Step: 1550... Loss: 2.1707... Val Loss: 2.0705\n",
      "Epoch: 31/50... Step: 1600... Loss: 2.1608... Val Loss: 2.0583\n",
      "Epoch: 32/50... Step: 1650... Loss: 2.1901... Val Loss: 2.0495\n",
      "Epoch: 33/50... Step: 1700... Loss: 2.1374... Val Loss: 2.0414\n",
      "Epoch: 34/50... Step: 1750... Loss: 2.1360... Val Loss: 2.0313\n",
      "Epoch: 35/50... Step: 1800... Loss: 2.1202... Val Loss: 2.0225\n",
      "Epoch: 36/50... Step: 1850... Loss: 2.1271... Val Loss: 2.0137\n",
      "Epoch: 37/50... Step: 1900... Loss: 2.1391... Val Loss: 2.0041\n",
      "Epoch: 38/50... Step: 1950... Loss: 2.1040... Val Loss: 1.9922\n",
      "Epoch: 39/50... Step: 2000... Loss: 2.1065... Val Loss: 1.9822\n",
      "Epoch: 40/50... Step: 2050... Loss: 2.1239... Val Loss: 1.9739\n",
      "Epoch: 41/50... Step: 2100... Loss: 2.0962... Val Loss: 1.9621\n",
      "Epoch: 42/50... Step: 2150... Loss: 2.0942... Val Loss: 1.9505\n",
      "Epoch: 43/50... Step: 2200... Loss: 2.0667... Val Loss: 1.9420\n",
      "Epoch: 44/50... Step: 2250... Loss: 2.0552... Val Loss: 1.9310\n",
      "Epoch: 45/50... Step: 2300... Loss: 2.0679... Val Loss: 1.9228\n",
      "Epoch: 46/50... Step: 2350... Loss: 2.0311... Val Loss: 1.9131\n",
      "Epoch: 47/50... Step: 2400... Loss: 2.0502... Val Loss: 1.9044\n",
      "Epoch: 48/50... Step: 2450... Loss: 2.0167... Val Loss: 1.8950\n",
      "Epoch: 49/50... Step: 2500... Loss: 2.0293... Val Loss: 1.8885\n",
      "Epoch: 50/50... Step: 2550... Loss: 2.0053... Val Loss: 1.8790\n",
      "Epoch: 50/50... Step: 2600... Loss: 1.9816... Val Loss: 1.8700\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAfQElEQVR4nO3de3TU5b3v8fd3JjdIQoBkJgkhIUDCJdwFigrKRUBQ66Xabo+tvey21tZ6tLWrtp7TvVdPj2e1p62n7lp366ldrWe7q1a8oK0KIqCIt4DcAyTIJZCQC7cQIOQyz/ljgqUYIJBJfjOTz2utrJn5zZPJ94HFhyfP7/k9P3POISIisc/ndQEiIhIZCnQRkTihQBcRiRMKdBGROKFAFxGJEwle/eCsrCxXWFjo1Y8XEYlJa9asqXfOBTp6z7NALywspLS01KsfLyISk8xs99ne05SLiEicUKCLiMQJBbqISJxQoIuIxAkFuohInFCgi4jECQW6iEic8Gwd+sWqqG1k8bp9DA+mURxMZ1gglZREv9dliYh4LuYCvay6gUeWVxBq38bdDPIH9KUomMbQrFQGpibRr08iGad9packkOT3kZzgI9HvI6n9MdFvmJm3HRIRiZCYC/RPTxjEvJJsdh04RkVt4z98vbPjACda2i7o83wGCT4fPl/7o4Hfd/agDzmHc39/dM5x6hYhBpgZ1v5iSGZffvqZ8YzNy+hKl0VEOiXmAh0gJdHPqJx+jMrp94n3Tra2ceRECw0nWjh8vIUjJ1o42tRKc1uIlrYQLa0hWtoczW0hmltDhJyjNeQIhcKPbe1fHXE4fGb4zDADw/BZ+LcE58Bx6jEc9q9squYzj67m/oWj+OfphfptQES6VUwG+rkkJ/gJpvsJpqd4XQr/9apivv/sen7y8hZWldfxi89OIDMt2euyRCROaZVLNxqYmsT//eIUfnz9GN7ecYAFD7/FqvJ6r8sSkTilQO9mZsaXLi/kxbumk9Enkdv/8B6/WV7hdVkiEocU6D1kdG4/Xvr2DGaOCPDwsvKzztOLiFwsBXoP6pPkZ+HYHJpbQ1QePO51OSISZxToPawomA6EL5ASEYkkBXoPKwqmAVBRp0AXkchSoPewjD6JBNKTKa9RoItIZCnQPVAcTNMIXUQiToHugaJgGjtqG3FOK11EJHIU6B4oCqbReLKV/Q1NXpciInFEge6Bj0+MaqWLiESQAt0DCnQR6Q7nDXQzyzez5WZWZmabzeyeDtpkmNlLZra+vc1Xuqfc+BBISyajTyLlCnQRiaDO7LbYCtznnFtrZunAGjNb6pzbclqbu4AtzrlPm1kA2GZmTzrnmruj6FhnZhQF0zRCF5GIOu8I3TlX7Zxb2/78KFAG5J3ZDEi38IbfacBBwv8RyFkUBcIrXUREIuWC5tDNrBCYBLx3xluPAKOBKmAjcI9zLtTB999hZqVmVlpXV3dRBceL4uw0Dhxr5uAx/RIjIpHR6UA3szRgEXCvc67hjLevBtYBg4CJwCNm9onbCTnnHnPOTXHOTQkEAl0oO/YN14lREYmwTgW6mSUSDvMnnXPPddDkK8BzLqwC2AmMilyZ8acooEAXkcjqzCoXAx4HypxzD52l2R7gqvb22cBI4KNIFRmP8vr3oU+in/Lao16XIiJxojOrXKYDtwMbzWxd+7EHgAIA59xvgZ8AfzSzjYAB9zvndK+1c/D5jOHBVI3QRSRizhvozrlVhEP6XG2qgPmRKqq3KAqk8f7Og16XISJxQleKeqg4O52qI000ntQKTxHpOgW6h4a3nxjVenQRiQQFuoeKs8OBri0ARCQSFOgeGjKwL4l+04lREYkIBbqHEvw+CjO10kVEIkOB7rHi7DQqtBZdRCJAge6xokAaew4ep6mlzetSRCTGKdA9NjyYRsjBrgPHvC5FRGKcAt1jxcF0AMprNI8uIl2jQPfYsEAqZtqkS0S6ToHusZREP/kD+lJRp0AXka5RoEeB4mAaFZpyEZEuUqBHgaJgGjvrj9Ha9ombPImIdJoCPQoUBdNobgtReeiE16WISAxToEeBovbb0ZXX6AIjEbl4CvQo8PH9RXViVES6QIEeBfqlJJLTL0VLF0WkSxToUaIomKZAF5EuUaBHibF5GWypauDIiRavSxGRGKVAjxLzSoK0hhwrttV6XYqIxCgFepSYmD+ArLQklm6p8boUEYlRCvQo4fcZc0dns2JbHSdbtZWuiFw4BXoUmVeSTePJVt796KDXpYhIDFKgR5HpRVn0SfSzdMt+r0sRkRikQI8iKYl+Zo4IsHRLDaGQ87ocEYkxCvQoM68km5qGk2zcd8TrUkQkxijQo8ycUUH8PmOJpl1E5AIp0KPMgNQkphYO0PJFEblgCvQoNK8kh+01jeyq142jRaTzFOhRaH5JNoBG6SJyQc4b6GaWb2bLzazMzDab2T1naTfLzNa1t1kZ+VJ7j/yBfRmVk65AF5EL0pkReitwn3NuNHApcJeZlZzewMz6A48C1zvnxgCfjXilvcz8kmxKdx/kQONJr0sRkRhx3kB3zlU759a2Pz8KlAF5ZzS7DXjOObenvZ12mOqi+WNyCDlYtlV/lCLSORc0h25mhcAk4L0z3hoBDDCzFWa2xsy+eJbvv8PMSs2stK6u7mLq7TXGDOrHoIwUTbuISKd1OtDNLA1YBNzrnGs44+0EYDJwLXA18CMzG3HmZzjnHnPOTXHOTQkEAl0oO/6ZGXNLsnmrvI4TzdqsS0TOr1OBbmaJhMP8Sefccx002Qu86pw75pyrB94EJkSuzN5pfkkOTS0h3irXbzMicn6dWeViwONAmXPuobM0exG4wswSzKwvMI3wXLt0wbRhA0lPSWCJpl1EpBMSOtFmOnA7sNHM1rUfewAoAHDO/dY5V2ZmrwIbgBDwe+fcpu4ouDdJ9PuYPTLIG1traQs5/D7zuiQRiWLnDXTn3CrgvEninPs58PNIFCV/N7ckm8Xrq/hwzyGmFA70uhwRiWK6UjTKzRwRIMFnvF6m5Ysicm4K9CiX0SeRTw0dyLIyzaOLyLkp0GPAVaOzKa9tZPcBbdYlImenQI8Bc0cHATTtIiLnpECPAUMyUykOpmnaRUTOSYEeI64anc37Ow9y5ESL16WISJRSoMeIeSVBWkOOldt11aiIdEyBHiMm5g9gYGqSpl1E5KwU6DHC7zNmjwyyfGstLW0hr8sRkSikQI8h80qCNDS1UrrrkNeliEgUUqDHkCuKAyT5fZp2EZEOKdBjSGpyApcNz+T1shqcc16XIyJRRoEeY+aODrLrwHF21OmqURH5Rwr0GDNndDaApl1E5BMU6DEmr38fSnL7sUzbAIjIGRToMWju6CCluw9y6Fiz16WISBRRoMeguSXZhBws36ZRuoj8nQI9Bo0dlEEwPZmluteoiJxGgR6DfD7juvGDWLqlhl31Wu0iImEK9Bh156xhJPiNX72+3etSRCRKKNBjVDA9ha9MH8qL66vYur/B63JEJAoo0GPYN64cRlpyAr9colG6iCjQY1r/vkl848phLN1Sw4d7tGGXSG+nQI9xX5k+lMzUJH7+2javSxERjynQY1xqcgJ3zS5i9Y4DvF1R73U5IuIhBXocuG1aAYMyUvjfr23TLowivZgCPQ6kJPq5Z24x6ysP62IjkV5MgR4nbr5kMEOzUvnlku20hTRKF+mNFOhxIsHv47vzRrCt5igvra/yuhwR8cB5A93M8s1suZmVmdlmM7vnHG2nmlmbmd0S2TKlM64dl0tJbj9+9upWGppavC5HRHpYZ0borcB9zrnRwKXAXWZWcmYjM/MDPwNei2yJ0lk+n/G/PjOO2qMn+fHiLV6XIyI97LyB7pyrds6tbX9+FCgD8jpoejewCNCerh6amN+fb80azqK1e1myeb/X5YhID7qgOXQzKwQmAe+dcTwPuAn47Xm+/w4zKzWz0rq6ugurVDrt7jnFlOT244HnN3Kg8aTX5YhID+l0oJtZGuER+L3OuTN3g/oVcL9zru1cn+Gce8w5N8U5NyUQCFx4tdIpSQk+HvqnCTScaOW/v7BJa9NFeolOBbqZJRIO8yedc8910GQK8JSZ7QJuAR41sxsjVqVcsFE5/fjOvBG8smk/i7XqRaRX6MwqFwMeB8qccw911MY5N9Q5V+icKwSeBb7lnHshopXKBbvjymFcUtCfH72wif1HmrwuR0S6WWdG6NOB24E5Zrau/esaM7vTzO7s5vqkC/w+45efm0hLm+P+RRs09SIS5xLO18A5twqwzn6gc+7LXSlIImtoVio/vGYU//LiZv7z/T18ftoQr0sSkW6iK0V7gS9MG8L0okwe/GsZew4c97ocEekmCvRewOczfn7LBPxmfO8v67XXi0icUqD3EoP69+Ffrx/D+7sO8viqj7wuR0S6gQK9F7n5kjzml2Tzi9e2s73mqNfliEiEKdB7EbPwXi/pKQl895l1tLSFvC5JRCJIgd7LZKUl8+BN49i0r4Ffv1HhdTkiEkEK9F5owdgcPjMpj98sr2B95WGvyxGRCFGg91L/ev0YgunJfPeZdTS1nHMLHhGJEQr0XiqjTyI/v2UCO+qO8dNXtnpdjohEgAK9F5tRnMVXZwzlj6t38fyHe70uR0S6SIHey/1g4SimDR3IDxZtZNO+I16XIyJdoEDv5RL9Ph79/CVkpSVzxxOl1OuGGCIxS4EuZKYl87vbJ3PgWDN3PblW69NFYpQCXQAYm5fBT28ex3s7D/LgX8u8LkdELsJ5t8+V3uOmSYPZtK+Bx1ftZMygfnx2Sr7XJYnIBdAIXf7BDxeO4vLhmfy3FzaxThcdicQUBbr8gwS/j0duu4RgejJf+P17LCur8bokEekkBbp8wsDUJP5y52UMzUrla0+U8u8rduj2dSIxQIEuHcrN6MMz37iM68YP4mevbuXep7VFgEi000lROas+SX7+7daJjMpJ5xdLtrGz/hi/u30yuRl9vC5NRDqgEbqck5lx1+wiHrt9CjtqG7n+kbdZs/uQ12WJSAcU6NIp80qyef6u6fRJ9HPrY+/w/97drXl1kSijQJdOG5GdzkvfnsGMoix+9MIm7ntmPSeaNa8uEi0U6HJBMvom8viXpvKduSN4ft0+bnr0bXYfOOZ1WSKCAl0ugs9n3DO3mD98eSrVR5q47tereH2L1quLeE2BLhdt9sggL989g4KBffnaE6X8+KXNNDS1eF2WSK+lQJcuyR/Yl0XfvJwvXFrAH1fvYs4vVvDMB5WEQjphKtLTFOjSZSmJfv7njeNYfFd4tP79RRu46dG3WbtHyxtFepICXSJm3OAMFn3zcn71TxOpPtLEZx5dzXefWUdtQ5PXpYn0CucNdDPLN7PlZlZmZpvN7J4O2nzezDa0f602swndU65EOzPjxkl5vPG9WXxz1nBeXl/N7F+s4N9X7OBkq5Y4inSnzozQW4H7nHOjgUuBu8ys5Iw2O4GZzrnxwE+AxyJbpsSatOQE7l8wiiXfuZLLhmfxs1e3Mv//vMnSLTW6IEmkm5w30J1z1c65te3PjwJlQN4ZbVY7505NmL4LDI50oRKbCrNS+f2XpvDEP3+KRL+Prz9Ryhf/8D7lNUe9Lk0k7lzQHLqZFQKTgPfO0eyrwCsXX5LEoytHBHjlniv4l+tKWFd5mAUPv8UDz2+k6vAJr0sTiRvW2V9/zSwNWAk86Jx77ixtZgOPAjOccwc6eP8O4A6AgoKCybt3777YuiWGHWg8ya9eL+epD/ZgGLdNK+Bbs4YT7JfidWkiUc/M1jjnpnT4XmcC3cwSgZeB15xzD52lzXjgeWChc277+T5zypQprrS09Lw/W+JX5cHjPPJGBc+u3UuCz7j90iHcOWs4WWnJXpcmErW6FOhmZsCfgIPOuXvP0qYAeAP4onNudWeKUqDLKbsPHOPfllXw/Id7SU7w8/lpBXz1iqHad12kA10N9BnAW8BGINR++AGgAMA591sz+z1wM3BqDqX1bD/wFAW6nGlHXSO/XlbOSxuqMeCGiXncOXMYxdnpXpcmEjW6POXSHRTocjaVB4/z+KqdPPXBHppaQswdHeQbM4cztXCg16WJeE6BLjHp4LFmnnhnF39avYtDx1uYWjiAu+cUc0VxFuGZQJHeR4EuMe14cytPf1DJ71Z+xP6GJibk9+fu2UVcNTqoYJdeR4EuceFkaxuL1uzj0RUV7D10gtG5/bh7ThFXj8nB71OwS++gQJe40tIW4sV1VTy6vIKP6o+R0y+FGyYN4uZLBjNCJ1AlzinQJS61hRyvbd7Ps2v2snJ7HW0hx9i8ftw0aTDXTxhEIF3r2SX+KNAl7tU3nmTxuiqe/3AfG/cdwe8zLhuWycJxOcwvyVG4S9xQoEuvUl5zlOc/3Mcrm/azs/4YPoOphQNZODaHBWNzycnQFgMSuxTo0is559hWc5S/bdzPq5uq2V7TCMAVxVncfukQ5owKkuDXPV4ktijQRYCK2kZeWl/F0x9Usr+hiUEZKdw2rYDPTc0nmK5Ru8QGBbrIaVrbQrxeVst/vLubVRX1JPiMBWNzuG1aAZcNy9Tadolq5wr0hJ4uRsRrCX4fC8bmsGBsDh/VNfLke3v4S2klL2+opjCzL5+bms8tkwdr1C4xRyN0EaCppY1XNlXz5/creX/nQRJ8xlWjg9z6qQKuKMrSXLtEDU25iFyAHXWNPP1BJc+u2cvBY81kpiaxcFwO140fxNTCgboqVTylQBe5CM2tId7YWsNLG6pZVlZDU0uIYHoy14zL5dMTcpmUPwCfwl16mAJdpIuOnWxl2dZaXl5fxYrtdTS3hsN9bkk280uyuWx4JskJfq/LlF5AgS4SQUebWlhWVsuSLftZsa2O481tpCUnMHNkgHmjsxmbl8GQzL4kat5duoECXaSbNLW08c6OAyzZsp+lW2qob2wGIMFnFGalUhRIozg7jRHZ6VxZHCCjb6LHFUusU6CL9IC2kKOsuoHtNUcpr22koraRHbWN7DpwjJCDRL8xoyiLa8blMr8kR+EuF0WBLuKhk61tbKlq4NVN+3l5QzX7Dp8g0W9cURzg2nG5LBibQ2qyLgmRzlGgi0QJ5xzr9x7hbxur+Wt7uKclJ/DpCYO4dWo+4wdn6EpVOScFukgUcs5RuvsQT39QycsbqmhqCTEqJ51bp+Zz46Q8+vdN8rpEiUIKdJEo19DU8vHGYRv2HiHJ72Pq0AHMHBFg1sggxcE0jdwFUKCLxJTNVUd4cV0VK7fVsa3mKACDMlKYOTLAlcUBLhueqdF7L6ZAF4lRVYdP8Ob2OlZur2NVeT1HT7ZiBmMHZXD58EwuL8piauEA+ibppGpvoUAXiQMtbSHWVx7m7YoDvL2jng/3HKKlzZHoNy4pGMDVY8I7SA7q38frUqUbKdBF4tDx5lY+2HWI1RX1rDhtemZifn8Wjs1h4dhcCjL7elylRJoCXaQX+KiukVc27eeVTdVs2tcAQEluP2aPCp9YnZTfX9sAxwEFukgvU3nwOK9sqmbplhrW7jlMW8iRnpLAFcVZzBoR5MoRAd0sO0Yp0EV6sSPHW3h7Rz0rttWycnsdNQ0nARiVk87MkQFmjggwZchAkhI0eo8FCnQRAcIXM23df5QV2+p4c3sdpbsP0tLmSE3yc9nwLGaNDDB3dLZG71GsS4FuZvnAE0AOEAIec849fEYbAx4GrgGOA192zq091+cq0EW813iyldUV9azcXseKbXXsO3wCgAn5/Znfvtd7kS5qiipdDfRcINc5t9bM0oE1wI3OuS2ntbkGuJtwoE8DHnbOTTvX5yrQRaKLc47y2kaWbqlhyeb9rN97BIChWalcPSaHWybnURRM97hKieiUi5m9CDzinFt62rHfASucc39uf70NmOWcqz7b5yjQRaLb/iNNLC0Lh/s7Ow7QGnJcUtCfz03J59rxuaSnaPtfL0Qs0M2sEHgTGOucazjt+MvAT51zq9pfLwPud86VnvH9dwB3ABQUFEzevXv3hfVERDxRd/QkL3y4j6dLK6mobaRPop9rxuVy8+Q8pg3N1I2ze9C5Ar3T1wubWRqwCLj39DA/9XYH3/KJ/ymcc48Bj0F4hN7Zny0i3gqkJ/P1K4fxtSuG8mHlYf5SWslL66tZtHYvmalJzCvJ5uqxOVyue6t6qlOBbmaJhMP8Sefccx002Qvkn/Z6MFDV9fJEJJqYhbcZuKRgAD+6roTlW+t4dfN+XlpfxVMfVJKenMDsUUHmlWQzoyiLAanaRKwnnTfQ21ewPA6UOeceOkuzxcC3zewpwidFj5xr/lxEYl/fpASuHZ/LteNzaWppY/WOel7dFL636uL1VZjBuLwMZhRlMaM4i8lDBmj03s06s8plBvAWsJHwskWAB4ACAOfcb9tD/xFgAeFli185c/78TDopKhKfWttCrN97mLfK61lVXs+HleErVfsk+plelMUtk/OYMypbFzJdJF1YJCKeOdrUwnsfHeSt8vD0TE3DSQamJnHDxEHcMnkwYwZleF1iTFGgi0hUaG0L8VZFPc+W7mXplhqa20KU5Pbjs1MGc8PEPAZqzv28FOgiEnUOHWtm8foq/rKmkk37Gkj0G3NGBbllcj6zRgZI1M6QHVKgi0hUK6tuYNGavbywbh/1jc1kpiZx46Q8rp8wiHF5Gfi0zv1jCnQRiQktbSFWbqvj2TV7Wba1hpY2x4C+iUwvyuLK4gAzirN6/R2ZInJhkYhId0v0+5hbks3ckmwOHWvmzfI63txez1vldby8IbwSengglTmjgtwwMY8xg/pp47DTaIQuIlHPOcf2mkbeKg/fMPvdjw7Q0uYYHkjlhol53DBxEEMyU70us0doykVE4sqhY828smk/L6zbx/s7DwJ/v5fqjOIsRuf0i9t5dwW6iMStqsMnWLy+ihfXVVFWHd5mKjM1icuLsphRlMn0oiwGD4ifm2Ur0EWkV9h/pIm3K+p5u6KeVRX11B4N325vWCCV+SU5zB+TzcTB/WN69K5AF5Fe59QNO1aV1/PG1lre/Si8p3sgPZm5o8N3Y7pseCYpibG1v4wCXUR6vSMnWlixrZYlm2tYsa2WY81tpCT6uGxYJleOCN8se2hWatSvmlGgi4ic5mRrG6t3HGDltvCqmZ31xwAYPKAPM0cEmD0yyPSiLPokRd/oXYEuInIOew4cZ2V5HSu31fHOjvqPR+8zigLMKwkyZ1Q2gfRkr8sEFOgiIp3W3BrivZ0HeH1LDa+X1bLv8AnMwssipw/PYvzgDCbk9ye7X4on9SnQRUQugnOOsuqjLCur4fWyGjZVNdAWCmdmTr+Uj8P90mGZTMrvmdUzCnQRkQg40dzGluojrKs8woa9h9mw98jH8+9ZaUlcNSq8bcGMbpx/114uIiIR0CfJz+QhA5k8ZODHx07tObN0Sw1/21jN06WVH8+/XzU6yKyRAXIzemZDMQW6iEgXDEhNat9PJo/m1hDv7zzI62U1LN0SnqYBGJWTzuxRQWaPDHJJQX8Summvd025iIh0g1MXNi3fWsvybbWU7jpEa8jRLyWBu+cU8/Urh13U52rKRUSkh5kZI7LTGZGdzjdmDqehqYW3y+tZvq2WnIzuWSGjQBcR6QH9UhJZOC6XheNyu+1n6KZ9IiJxQoEuIhInFOgiInFCgS4iEicU6CIicUKBLiISJxToIiJxQoEuIhInPLv038zqgN0X+e1ZQH0Ey4l26m/86k19BfU3EoY45wIdveFZoHeFmZWebS+DeKT+xq/e1FdQf7ubplxEROKEAl1EJE7EaqA/5nUBPUz9jV+9qa+g/narmJxDFxGRT4rVEbqIiJxBgS4iEidiLtDNbIGZbTOzCjP7gdf1RJqZ/cHMas1s02nHBprZUjMrb38c4GWNkWJm+Wa23MzKzGyzmd3Tfjxe+5tiZu+b2fr2/v64/Xhc9hfAzPxm9qGZvdz+Op77usvMNprZOjMrbT/Wo/2NqUA3Mz/wG2AhUAL8FzMr8baqiPsjsOCMYz8AljnnioFl7a/jQStwn3NuNHApcFf732e89vckMMc5NwGYCCwws0uJ3/4C3AOUnfY6nvsKMNs5N/G0tec92t+YCnTgU0CFc+4j51wz8BRwg8c1RZRz7k3g4BmHbwD+1P78T8CNPVpUN3HOVTvn1rY/P0r4H34e8dtf55xrbH+Z2P7liNP+mtlg4Frg96cdjsu+nkOP9jfWAj0PqDzt9d72Y/Eu2zlXDeEQBIIe1xNxZlYITALeI4772z4FsQ6oBZY65+K5v78Cvg+ETjsWr32F8H/OS8xsjZnd0X6sR/sbazeJtg6Oad1ljDOzNGARcK9zrsGso7/m+OCcawMmmll/4HkzG+t1Td3BzK4Dap1za8xsltf19JDpzrkqMwsCS81sa08XEGsj9L1A/mmvBwNVHtXSk2rMLBeg/bHW43oixswSCYf5k86559oPx21/T3HOHQZWED5fEo/9nQ5cb2a7CE+NzjGz/yA++wqAc66q/bEWeJ7wFHGP9jfWAv0DoNjMhppZEnArsNjjmnrCYuBL7c+/BLzoYS0RY+Gh+ONAmXPuodPeitf+BtpH5phZH2AusJU47K9z7ofOucHOuULC/07fcM59gTjsK4CZpZpZ+qnnwHxgEz3c35i7UtTMriE8N+cH/uCce9DjkiLKzP4MzCK87WYN8K/AC8AzQAGwB/isc+7ME6cxx8xmAG8BG/n7POsDhOfR47G/4wmfGPMTHkw945z7H2aWSRz295T2KZfvOeeui9e+mtkwwqNyCE9l/6dz7sGe7m/MBbqIiHQs1qZcRETkLBToIiJxQoEuIhInFOgiInFCgS4iEicU6CIicUKBLiISJ/4/myq5j+HA7JEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size = 64\n",
    "# seq_length = 160 # max length of sequence\n",
    "n_epochs = 50 \n",
    "\n",
    "for n_hidden in n_hidden_list:\n",
    "  for n_layers in n_layers_list:\n",
    "    for seq_length in seq_length_list:\n",
    "      model_name = \"model_{}_{}_{}_{}\".format(n_hidden, n_layers, seq_length, n_epochs)\n",
    "      print(\"********** {} ************\".format(model_name))\n",
    "      hist_loss = []\n",
    "      hist_val_loss = []\n",
    "\n",
    "      net = CharRNN(chars, n_hidden, n_layers)\n",
    "      print(net)\n",
    "      # train the model\n",
    "      train(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=50)\n",
    "      \n",
    "      pyplot.plot(hist_loss)\n",
    "      pyplot.show()\n",
    "\n",
    "      pyplot.plot(hist_val_loss)\n",
    "      pyplot.show()\n",
    "      \n",
    "      checkpoint = {'n_hidden': net.n_hidden,\n",
    "              'n_layers': net.n_layers,\n",
    "              'state_dict': net.state_dict(),\n",
    "              'tokens': net.chars}\n",
    "\n",
    "      with open(output_folder_models+model_name, 'wb') as f:\n",
    "          torch.save(checkpoint, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BeDuez0rLkc-"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "461820"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.CharRNN"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "od4Qn6sJLlEt"
   },
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8bhdMUkacvH9"
   },
   "outputs": [],
   "source": [
    "def predict(net, char, h=None, top_k=None):\n",
    "        ''' Given a character, predict the next character.\n",
    "            Returns the predicted character and the hidden state.\n",
    "        '''\n",
    "        \n",
    "        # tensor inputs\n",
    "        x = np.array([[net.char2int[char]]])\n",
    "        x = one_hot_encode(x, len(net.chars))\n",
    "        inputs = torch.from_numpy(x)\n",
    "        \n",
    "        if(train_on_gpu):\n",
    "            inputs = inputs.cuda()\n",
    "        \n",
    "        # detach hidden state from history\n",
    "        h = tuple([each.data for each in h])\n",
    "        # get the output of the model\n",
    "        out, h = net(inputs, h)\n",
    "\n",
    "        # get the character probabilities\n",
    "        # apply softmax to get p probabilities for the likely next character giving x\n",
    "        p = F.softmax(out, dim=1).data\n",
    "        if(train_on_gpu):\n",
    "            p = p.cpu() # move to cpu\n",
    "        \n",
    "        \n",
    "        # keep probability to use later\n",
    "        prob = p.numpy().squeeze()\n",
    "        char_index = np.argmax(prob)\n",
    "        char_pred = net.int2char[char_index]\n",
    "        prob_pred = prob[char_index]\n",
    "        return char_pred, h, prob_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 364
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 245,
     "status": "ok",
     "timestamp": 1566772681834,
     "user": {
      "displayName": "Dmitry Vengertsev",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDYD27Z8EhBcFDMesTklMQKc27chgzd92EokKwqJw=s64",
      "userId": "11039444983797551480"
     },
     "user_tz": 360
    },
    "id": "i-98qMhKsEXu",
    "outputId": "ff923b23-1e84-4857-92c7-7effd52ffbe3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ch = b, char = e, prob = 0.30611371994018555\n",
      "ch = u, char = l, prob = 0.5654222965240479\n",
      "ch = t, char =  , prob = 0.5635764002799988\n",
      "ch =  , char = a, prob = 0.1304537057876587\n",
      "ch = t, char = h, prob = 0.6685751080513\n",
      "ch = o, char =  , prob = 0.925091028213501\n",
      "ch =  , char = t, prob = 0.14843431115150452\n",
      "ch = s, char = a, prob = 0.27939751744270325\n",
      "ch = p, char = e, prob = 0.5872915387153625\n",
      "ch = e, char = a, prob = 0.9448623657226562\n",
      "ch = a, char = k, prob = 0.9943780303001404\n",
      "ch = k, char =  , prob = 0.919374942779541\n",
      "ch =  , char = o, prob = 0.5279476046562195\n",
      "ch = s, char = o, prob = 0.3095758259296417\n",
      "ch = e, char = e, prob = 0.3643374443054199\n",
      "ch = r, char = v, prob = 0.669789731502533\n",
      "ch = i, char = o, prob = 0.886081337928772\n",
      "ch = o, char = u, prob = 0.9984220266342163\n",
      "ch = u, char = s, prob = 0.9993937015533447\n",
      "time taken to predict: 0.02683401107788086\n"
     ]
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "prime = 'but to speak seriou'\n",
    "chars = [ch for ch in prime]\n",
    "h = net.init_hidden(1)\n",
    "for ch in prime:\n",
    "  char, h, prob = predict(net, ch, h)\n",
    "  print(\"ch = {}, char = {}, prob = {}\".format(ch, char, prob))\n",
    "\n",
    "print(\"time taken to predict: {}\".format(time.time()-t1))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 261,
     "status": "ok",
     "timestamp": 1566771584761,
     "user": {
      "displayName": "Dmitry Vengertsev",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDYD27Z8EhBcFDMesTklMQKc27chgzd92EokKwqJw=s64",
      "userId": "11039444983797551480"
     },
     "user_tz": 360
    },
    "id": "1BFa_SPpsElz",
    "outputId": "a2d5fda6-53de-4a0c-b99b-ce964292baf7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.833333333333332"
      ]
     },
     "execution_count": 134,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.015*1.4*1000000/60/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D6DDA0WQ7OW9"
   },
   "outputs": [],
   "source": [
    "# LOADING \n",
    "# Here we have loaded in a model that trained over 20 epochs `rnn_20_epoch.net`\n",
    "with open('rnn_20_epoch.net', 'rb') as f:\n",
    "    checkpoint = torch.load(f)\n",
    "    \n",
    "loaded = CharRNN(checkpoint['tokens'], n_hidden=checkpoint['n_hidden'], n_layers=checkpoint['n_layers'])\n",
    "loaded.load_state_dict(checkpoint['state_dict'])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "train_char_rnn.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
